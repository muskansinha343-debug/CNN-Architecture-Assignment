{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN Architecture | Assignment\n"
      ],
      "metadata": {
        "id": "wLjVmUiSOlUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?"
      ],
      "metadata": {
        "id": "2bjEY_PAO1et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans. Role of Filters and Feature Maps in Convolutional Neural Networks (CNN)\n",
        "\n",
        "In a Convolutional Neural Network (CNN), filters (also called kernels) and feature maps play a crucial role in extracting important information from an input image.\n",
        "\n",
        "A filter is a small matrix of numbers (for example, 3Ã—3 or 5Ã—5) that slides over the input image. Its purpose is to detect specific visual patterns such as edges, corners, textures, or shapes. During convolution, the filter performs element-wise multiplication with a portion of the image and sums the result to produce a single value. This process is repeated across the entire image. Each filter is trained to recognize a particular type of feature, such as vertical edges, horizontal lines, or curves.\n",
        "\n",
        "When a filter is applied to the image, it produces a feature map. A feature map is the output matrix that shows where a particular feature is present in the image and how strongly it appears. Bright or high-value areas in a feature map indicate strong detection of that feature, while low values indicate weak or no detection. If multiple filters are used, multiple feature maps are created, each representing a different type of detected feature.\n",
        "\n",
        "Together, filters and feature maps enable CNNs to learn hierarchical patterns in data. In the early layers, filters detect simple features like edges and textures. In deeper layers, they combine these simple features to detect more complex patterns such as objects or faces. Thus, filters act as feature detectors, and feature maps store the extracted information that allows the CNN to understand and classify images effectively."
      ],
      "metadata": {
        "id": "m_gxUDsNO8rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n"
      ],
      "metadata": {
        "id": "nXmwf2brPl2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans. Padding and Stride in Convolutional Neural Networks (CNNs)\n",
        "\n",
        "In Convolutional Neural Networks (CNNs), padding and stride are two important parameters that control how a filter moves across an input image and how the size of the output feature map is determined.\n",
        "\n",
        "1. Padding\n",
        "\n",
        "Padding means adding extra pixels (usually zeros) around the border of the input image before applying the convolution filter. The main purpose of padding is to control the size of the output feature map and to make sure that important information at the edges of the image is not lost.\n",
        "\n",
        "There are two common types of padding:\n",
        "\n",
        "Valid padding: No padding is added, so the output feature map becomes smaller.\n",
        "\n",
        "Same padding: Padding is added so that the output feature map has the same height and width as the input.\n",
        "\n",
        "Padding helps preserve spatial information and allows deeper CNNs to be built without reducing the image size too quickly.\n",
        "\n",
        "2. Stride\n",
        "\n",
        "Stride refers to the number of pixels the filter moves at each step while scanning the image.\n",
        "\n",
        "A stride of 1 means the filter moves one pixel at a time.\n",
        "\n",
        "A stride of 2 means the filter moves two pixels at a time.\n",
        "\n",
        "A larger stride causes the output feature map to become smaller because the filter skips more positions while moving across the image.\n",
        "\n",
        "Effect on Output Feature Map Size\n",
        "\n",
        "Padding and stride directly affect the dimensions of the output feature map. The output size is calculated using the formula:\n",
        "\n",
        "Output Size\n",
        "=\n",
        "(\n",
        "ð‘\n",
        "âˆ’\n",
        "ð¹\n",
        "+\n",
        "2\n",
        "ð‘ƒ\n",
        ")\n",
        "ð‘†\n",
        "+\n",
        "1\n",
        "Output Size=\n",
        "S\n",
        "(Nâˆ’F+2P)\n",
        "\tâ€‹\n",
        "\n",
        "+1\n",
        "\n",
        "Where:\n",
        "\n",
        "ð‘\n",
        "N = input size\n",
        "\n",
        "ð¹\n",
        "F = filter (kernel) size\n",
        "\n",
        "ð‘ƒ\n",
        "P = padding\n",
        "\n",
        "ð‘†\n",
        "S = stride\n",
        "\n",
        "From this formula:\n",
        "\n",
        "Increasing padding increases the output size.\n",
        "\n",
        "Increasing stride decreases the output size.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Padding helps control edge information and maintain spatial dimensions, while stride controls how much the filter moves and how much the feature map is downsampled. By choosing appropriate padding and stride values, CNNs can efficiently extract features while managing the size of feature maps."
      ],
      "metadata": {
        "id": "fYQ7nB5cPw_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?"
      ],
      "metadata": {
        "id": "etVYdD9cQETE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans. Receptive Field in Convolutional Neural Networks (CNNs)\n",
        "\n",
        "In the context of Convolutional Neural Networks (CNNs), the receptive field refers to the region of the input image that influences the activation of a particular neuron (or feature map value). In simple terms, it is the portion of the original image that a neuron in a CNN layer can â€œseeâ€ and respond to.\n",
        "\n",
        "For example, a neuron in the first convolutional layer may only look at a small area such as a 3Ã—3 or 5Ã—5 region of the image. As we move deeper into the network, neurons begin to look at larger portions of the input because the outputs of earlier layers are combined. Thus, deeper layers have a larger receptive field.\n",
        "\n",
        "Why Receptive Field is Important for Deep Architectures\n",
        "\n",
        "The receptive field is important because it determines how much context a neuron can capture from the input image.\n",
        "\n",
        "In early layers, a small receptive field allows the network to detect simple features like edges and textures.\n",
        "\n",
        "In deeper layers, a larger receptive field enables the network to recognize more complex and global patterns such as shapes, objects, or faces.\n",
        "\n",
        "By gradually increasing the receptive field through multiple convolutional and pooling layers, deep CNNs can build a hierarchical understanding of the image. This allows the network to combine low-level features into high-level representations, which is essential for accurate image classification, object detection, and recognition."
      ],
      "metadata": {
        "id": "eCszSsJzQKe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN."
      ],
      "metadata": {
        "id": "B3EPpup7QdIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans. Influence of Filter Size and Stride on the Number of Parameters in a CNN\n",
        "\n",
        "In a Convolutional Neural Network (CNN), the number of parameters mainly depends on the filter (kernel) size, the number of filters, and the number of input channels. Stride, on the other hand, does not directly change the number of parameters but affects how often the filter is applied and the size of the output feature maps.\n",
        "\n",
        "1. Effect of Filter Size\n",
        "\n",
        "The filter size determines how many weights each filter contains.\n",
        "For a filter of size\n",
        "ð¹\n",
        "Ã—\n",
        "ð¹\n",
        "FÃ—F applied to an input with\n",
        "ð¶\n",
        "C channels, the number of parameters in one filter is:\n",
        "\n",
        "ð¹\n",
        "Ã—\n",
        "ð¹\n",
        "Ã—\n",
        "ð¶\n",
        "FÃ—FÃ—C\n",
        "\n",
        "If the CNN layer has\n",
        "ð¾\n",
        "K such filters, then the total number of parameters is:\n",
        "\n",
        "ð¹\n",
        "Ã—\n",
        "ð¹\n",
        "Ã—\n",
        "ð¶\n",
        "Ã—\n",
        "ð¾\n",
        "FÃ—FÃ—CÃ—K\n",
        "\n",
        "So, increasing the filter size (for example, from 3Ã—3 to 5Ã—5) increases the number of parameters significantly, which leads to higher computational cost and a greater risk of overfitting.\n",
        "\n",
        "2. Effect of Stride\n",
        "\n",
        "Stride controls how far the filter moves at each step. However, stride does not change the number of weights in the filters. Therefore, stride does not directly affect the number of parameters.\n",
        "\n",
        "What stride does affect is:\n",
        "\n",
        "The size of the output feature map\n",
        "\n",
        "The number of convolution operations\n",
        "\n",
        "A larger stride produces smaller feature maps, which reduces computation in later layers and decreases memory usage, but it does not change how many learnable parameters exist in the convolution layer.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Filter size has a direct and strong impact on the number of parameters in a CNN.\n",
        "\n",
        "Stride affects the output size and computation, but not the number of learnable parameters.\n",
        "\n",
        "Choosing appropriate filter sizes and stride values helps balance model complexity, efficiency, and performance."
      ],
      "metadata": {
        "id": "ZVK6tEi6Qo4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n"
      ],
      "metadata": {
        "id": "DbXeh3f9Q2bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans. Comparison of CNN Architectures: LeNet, AlexNet, and VGG\n",
        "\n",
        "LeNet, AlexNet, and VGG are three important Convolutional Neural Network (CNN) architectures that played a major role in the development of deep learning for image recognition. They differ mainly in terms of depth, filter sizes, and performance.\n",
        "\n",
        "1. LeNet\n",
        "\n",
        "LeNet was one of the earliest CNN architectures, developed for handwritten digit recognition. It is a shallow network with only a few convolutional and pooling layers. It uses small filter sizes such as 5Ã—5 and has a relatively small number of parameters. LeNet performs well for simple tasks like digit classification (for example, MNIST) but is not suitable for complex, large-scale image recognition problems.\n",
        "\n",
        "2. AlexNet\n",
        "\n",
        "AlexNet is a much deeper network compared to LeNet and was designed for large-scale image classification. It introduced the use of larger filters such as 11Ã—11 and 5Ã—5 in early layers and used ReLU activation, dropout, and GPU training to improve performance. AlexNet achieved a major breakthrough by significantly improving accuracy on the ImageNet dataset, making deep CNNs popular for real-world applications.\n",
        "\n",
        "3. VGG\n",
        "\n",
        "VGG is even deeper than AlexNet, with architectures having 16 or 19 layers. Instead of large filters, VGG uses very small 3Ã—3 filters stacked together. This increases the depth while keeping the filter size small, allowing the network to learn complex patterns more effectively. VGG provides higher accuracy than AlexNet but requires much more memory and computation due to its large number of parameters.  \n",
        "| Feature     | LeNet                 | AlexNet                          | VGG                      |\n",
        "| ----------- | --------------------- | -------------------------------- | ------------------------ |\n",
        "| Depth       | Shallow (few layers)  | Medium deep                      | Very deep (16â€“19 layers) |\n",
        "| Filter Size | Mainly 5Ã—5            | Large (11Ã—11, 5Ã—5)               | Small (3Ã—3)              |\n",
        "| Performance | Good for simple tasks | High for large datasets          | Very high accuracy       |\n",
        "| Computation | Low                   | Medium                           | Very high                |\n",
        "| Typical Use | Digit recognition     | Large-scale image classification | Advanced vision tasks    |\n"
      ],
      "metadata": {
        "id": "firbVbT_Q7Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation."
      ],
      "metadata": {
        "id": "3pHwuLb6RNlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values (0â€“255 â†’ 0â€“1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Reshape data for CNN (28x28x1)\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "model = models.Sequential()\n",
        "\n",
        "# First Convolutional Layer\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "# Second Convolutional Layer\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "# Flattening\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Fully Connected Layer\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkcVwfyYRbr5",
        "outputId": "018efb13-c362-4ca4-c830-17b25801efff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 55ms/step - accuracy: 0.8970 - loss: 0.3565 - val_accuracy: 0.9844 - val_loss: 0.0467\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 56ms/step - accuracy: 0.9832 - loss: 0.0531 - val_accuracy: 0.9878 - val_loss: 0.0377\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 55ms/step - accuracy: 0.9894 - loss: 0.0329 - val_accuracy: 0.9892 - val_loss: 0.0326\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.9929 - loss: 0.0230 - val_accuracy: 0.9915 - val_loss: 0.0272\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.9949 - loss: 0.0173 - val_accuracy: 0.9929 - val_loss: 0.0237\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9906 - loss: 0.0303\n",
            "Test Accuracy: 0.992900013923645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.\n"
      ],
      "metadata": {
        "id": "-_rM7K5lSdQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values (0â€“255 â†’ 0â€“1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "print(\"Training shape:\", X_train.shape)\n",
        "print(\"Testing shape:\", X_test.shape)\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolution Block 1\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "# Convolution Block 2\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "# Convolution Block 3\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "\n",
        "# Flatten and Dense Layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIxZK9C0S6nb",
        "outputId": "48f3db2c-fcbf-4352-d44d-86656fc9d96e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Training shape: (50000, 32, 32, 3)\n",
            "Testing shape: (10000, 32, 32, 3)\n",
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 86ms/step - accuracy: 0.3345 - loss: 1.8054 - val_accuracy: 0.5099 - val_loss: 1.3755\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 88ms/step - accuracy: 0.5599 - loss: 1.2428 - val_accuracy: 0.6047 - val_loss: 1.1173\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 95ms/step - accuracy: 0.6311 - loss: 1.0496 - val_accuracy: 0.6604 - val_loss: 0.9673\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 85ms/step - accuracy: 0.6727 - loss: 0.9401 - val_accuracy: 0.6619 - val_loss: 0.9725\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 86ms/step - accuracy: 0.7034 - loss: 0.8581 - val_accuracy: 0.6746 - val_loss: 0.9506\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 87ms/step - accuracy: 0.7153 - loss: 0.8084 - val_accuracy: 0.6841 - val_loss: 0.9160\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 88ms/step - accuracy: 0.7457 - loss: 0.7348 - val_accuracy: 0.7003 - val_loss: 0.8748\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 88ms/step - accuracy: 0.7649 - loss: 0.6771 - val_accuracy: 0.7072 - val_loss: 0.8465\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 84ms/step - accuracy: 0.7763 - loss: 0.6377 - val_accuracy: 0.7187 - val_loss: 0.8387\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 85ms/step - accuracy: 0.7919 - loss: 0.5906 - val_accuracy: 0.7203 - val_loss: 0.8406\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.7245 - loss: 0.8376\n",
            "Test Accuracy: 0.720300018787384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n"
      ],
      "metadata": {
        "id": "u1r6EKYVTVNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "# Transform: convert images to tensors and normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128) # Corrected input features for fc1\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)   # flatten\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(\"Test Accuracy:\", accuracy, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PitLBSN2TjGk",
        "outputId": "f8af2adb-81fe-45c4-c1e6-44f77411059d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.1474\n",
            "Epoch [2/5], Loss: 0.0468\n",
            "Epoch [3/5], Loss: 0.0315\n",
            "Epoch [4/5], Loss: 0.0243\n",
            "Epoch [5/5], Loss: 0.0167\n",
            "Test Accuracy: 99.1 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model"
      ],
      "metadata": {
        "id": "YFhQ4vIxUuHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "\n",
        "# --- Start of Fix ---\n",
        "# Create dummy directories if they don't exist to prevent FileNotFoundError.\n",
        "# In a real scenario, these directories should be populated with your actual images,\n",
        "# organized into subdirectories representing different classes.\n",
        "dummy_train_class_path = \"dataset/train/dummy_class\"\n",
        "dummy_validation_class_path = \"dataset/validation/dummy_class\"\n",
        "\n",
        "os.makedirs(dummy_train_class_path, exist_ok=True)\n",
        "os.makedirs(dummy_validation_class_path, exist_ok=True)\n",
        "# --- End of Fix ---\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    \"dataset/train\",\n",
        "    target_size=(150,150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    \"dataset/validation\",\n",
        "    target_size=(150,150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Determine the number of classes. If no classes are found (e.g., directories are empty),\n",
        "# num_classes will be 0, which would crash the model definition.\n",
        "# We'll use the number detected by flow_from_directory.\n",
        "num_classes = train_data.num_classes\n",
        "\n",
        "if num_classes == 0:\n",
        "    print(\"Warning: No image classes found in 'dataset/train'. Model cannot be built with 0 output classes.\")\n",
        "    print(\"Please ensure 'dataset/train' contains subdirectories with your training images.\")\n",
        "\n",
        "# Only proceed with model building and training if classes are detected.\n",
        "if num_classes > 0:\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
        "    model.add(layers.MaxPooling2D(2,2))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(2,2))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(2,2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Only fit if data generators are not empty (i.e., contain actual images)\n",
        "    if train_data.n > 0 and val_data.n > 0:\n",
        "        print(f\"Found {train_data.n} training images belonging to {train_data.num_classes} classes.\")\n",
        "        print(f\"Found {val_data.n} validation images belonging to {val_data.num_classes} classes.\")\n",
        "        model.fit(\n",
        "            train_data,\n",
        "            epochs=10,\n",
        "            validation_data=val_data\n",
        "        )\n",
        "        loss, accuracy = model.evaluate(val_data)\n",
        "        print(\"Validation Accuracy:\", accuracy)\n",
        "    else:\n",
        "        print(\"Skipping model training and evaluation: No actual images found in the specified directories.\")\n",
        "        print(\"Please add images to 'dataset/train/dummy_class' and 'dataset/validation/dummy_class' (or your custom class subdirectories).\")\n",
        "else:\n",
        "    print(\"Model building and training skipped due to no classes found in data directories.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeAdXTOvUx7w",
        "outputId": "67620c77-85fa-43c2-d137-04925b10740f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 1 classes.\n",
            "Found 0 images belonging to 1 classes.\n",
            "Skipping model training and evaluation: No actual images found in the specified directories.\n",
            "Please add images to 'dataset/train/dummy_class' and 'dataset/validation/dummy_class' (or your custom class subdirectories).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into â€œNormalâ€\n",
        "and â€œPneumoniaâ€ categories. Describe your end-to-end approachâ€“from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit"
      ],
      "metadata": {
        "id": "gv2hrCymVHkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chest_xray/\n",
        "#  â”œâ”€â”€ train/\n",
        "#  â”‚     â”œâ”€â”€ NORMAL/\n",
        "#  â”‚     â””â”€â”€ PNEUMONIA/\n",
        "#  â”œâ”€â”€ val/\n",
        "#  â”‚     â”œâ”€â”€ NORMAL/\n",
        "#  â”‚     â””â”€â”€ PNEUMONIA/\n",
        "#  â””â”€â”€ test/\n",
        "#        â”œâ”€â”€ NORMAL/\n",
        "#        â””â”€â”€ PNEUMONIA/\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# --- Start of Fix: Create dummy directories for demonstration ---\n",
        "# In a real scenario, you would have your actual dataset organized like this.\n",
        "# These lines are for preventing FileNotFoundError during execution if the dataset isn't present.\n",
        "base_dir = \"chest_xray\"\n",
        "os.makedirs(os.path.join(base_dir, \"train\", \"NORMAL\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_dir, \"train\", \"PNEUMONIA\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_dir, \"val\", \"NORMAL\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_dir, \"val\", \"PNEUMONIA\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_dir, \"test\", \"NORMAL\"), exist_ok=True) # Assuming test is also needed\n",
        "os.makedirs(os.path.join(base_dir, \"test\", \"PNEUMONIA\"), exist_ok=True) # Assuming test is also needed\n",
        "# --- End of Fix ---\n",
        "\n",
        "img_size = 224\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    os.path.join(base_dir, \"train\"),\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    os.path.join(base_dir, \"val\"),\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "# Check if data generators have found any images\n",
        "if train_data.n == 0 or val_data.n == 0:\n",
        "    print(\"Warning: No images found in the 'chest_xray/train' or 'chest_xray/val' directories.\")\n",
        "    print(\"Please populate these directories with actual images for training and validation.\")\n",
        "    print(\"Skipping model building and training.\")\n",
        "else:\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=(224,224,3),\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\"\n",
        "    )\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        epochs=10,\n",
        "        validation_data=val_data\n",
        "    )\n",
        "    model.save(\"pneumonia_model.h5\")\n",
        "\n",
        "    # Streamlit app code - this part would typically be in a separate .py file\n",
        "    # to be run with `streamlit run app.py`\n",
        "    # For demonstration within Colab, it won't be interactive directly.\n",
        "    print(\"\\n--- Streamlit App Code (for external deployment) ---\")\n",
        "    print(\"import streamlit as st\")\n",
        "    print(\"import tensorflow as tf\")\n",
        "    print(\"from PIL import Image\")\n",
        "    print(\"import numpy as np\")\n",
        "    print(\"\\n# Ensure the model file exists\")\n",
        "    print(\"model_path = \\\"pneumonia_model.h5\\\"\")\n",
        "    print(\"if not os.path.exists(model_path):\")\n",
        "    print(\"    st.error(\\\"Model file not found! Please train the model first.\\\")\")\n",
        "    print(\"else:\")\n",
        "    print(\"    model = tf.keras.models.load_model(model_path)\")\n",
        "    print(\"\\n    st.title(\\\"ðŸ©º Chest X-Ray Pneumonia Detector\\\")\")\n",
        "    print(\"\\n    uploaded_file = st.file_uploader(\\\"Upload Chest X-ray Image\\\", type=[\\\"jpg\\\",\\\"png\\\",\\\"jpeg\\\"])\")\n",
        "    print(\"\\n    if uploaded_file:\")\n",
        "    print(\"        image = Image.open(uploaded_file).resize((224,224))\")\n",
        "    print(\"        st.image(image, caption=\\\"Uploaded X-ray\\\")\")\n",
        "    print(\"\\n        img = np.array(image)/255.0\")\n",
        "    print(\"        img = img.reshape(1,224,224,3)\")\n",
        "    print(\"\\n        prediction = model.predict(img)[0][0]\")\n",
        "    print(\"\\n        if prediction > 0.5:\")\n",
        "    print(\"            st.error(\\\"ðŸ¦  Pneumonia Detected\\\")\")\n",
        "    print(\"        else:\")\n",
        "    print(\"            st.success(\\\"âœ… Normal Chest X-ray\\\")\")\n",
        "    print(\"\\n        st.write(\\\"Confidence:\\\", prediction)\")\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktLpZFP5VOBc",
        "outputId": "33b3590d-24fd-43ae-9879-2a3f459d8415"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 2 classes.\n",
            "Found 0 images belonging to 2 classes.\n",
            "Warning: No images found in the 'chest_xray/train' or 'chest_xray/val' directories.\n",
            "Please populate these directories with actual images for training and validation.\n",
            "Skipping model building and training.\n"
          ]
        }
      ]
    }
  ]
}